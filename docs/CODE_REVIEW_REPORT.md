# TASA SatNet Pipeline - ä»£ç¢¼å¯©æŸ¥å ±å‘Š

**å¯©æŸ¥æ—¥æœŸ**: 2025-10-08
**å¯©æŸ¥è€…**: Claude Code (Senior Code Reviewer Agent)
**å¯©æŸ¥ç¯„åœ**: æ ¸å¿ƒæ¨¡çµ„èˆ‡æ¸¬è©¦å¥—ä»¶
**å°ˆæ¡ˆç‰ˆæœ¬**: feat/k8s-deployment-verification åˆ†æ”¯

---

## ğŸ“Š åŸ·è¡Œæ‘˜è¦

### ç¸½é«”è©•åˆ†: **8.2/10** â­â­â­â­

| é¡åˆ¥ | è©•åˆ† | ç‹€æ…‹ |
|------|------|------|
| **TDD åˆè¦æ€§** | 9.5/10 | âœ… å„ªç§€ |
| **ä»£ç¢¼å“è³ª** | 8.0/10 | âœ… è‰¯å¥½ |
| **æ•ˆèƒ½å„ªåŒ–** | 7.5/10 | âš ï¸ å¯æ”¹é€² |
| **æ¶æ§‹è¨­è¨ˆ** | 8.5/10 | âœ… è‰¯å¥½ |
| **å®‰å…¨æ€§** | 7.0/10 | âš ï¸ éœ€åŠ å¼· |
| **æ–‡æª”å®Œæ•´æ€§** | 9.0/10 | âœ… å„ªç§€ |

### é—œéµç™¼ç¾
- âœ… **å„ªç§€çš„ TDD å¯¦è¸**: 28/28 æ¸¬è©¦é€šéï¼Œ98% è¦†è“‹ç‡
- âœ… **æ¸…æ™°çš„æ¶æ§‹**: æ¨¡çµ„åˆ†é›¢è‰¯å¥½ï¼Œè·è²¬æ˜ç¢º
- âœ… **ç”Ÿç”¢å°±ç·’**: K8s éƒ¨ç½²é©—è­‰é€šé
- âš ï¸ **é¡å‹æç¤ºä¸å®Œæ•´**: éƒ¨åˆ†å‡½æ•¸ç¼ºå°‘å®Œæ•´çš„é¡å‹æ¨™è¨»
- âš ï¸ **éŒ¯èª¤è™•ç†**: é‚Šç•Œæ¢ä»¶è™•ç†å¯ä»¥æ›´åš´è¬¹
- âš ï¸ **æ•ˆèƒ½ç“¶é ¸**: å¤§è¦æ¨¡æ•¸æ“šè™•ç†æ™‚å¯èƒ½å­˜åœ¨è¨˜æ†¶é«”å•é¡Œ

---

## ğŸ“ å„æª”æ¡ˆè©³ç´°å¯©æŸ¥

### 1. `scripts/parse_oasis_log.py` - OASIS æ—¥èªŒè§£æå™¨

**è©•åˆ†**: 8.0/10

#### âœ… å„ªé»
1. **æ­£å‰‡è¡¨é”å¼è¨­è¨ˆè‰¯å¥½**
   - æ”¯æ´å¤§å°å¯«ä¸æ•æ„ŸåŒ¹é…
   - è™•ç†å¤šé¤˜ç©ºç™½å­—ç¬¦
   - æº–ç¢ºæå–æ™‚é–“æˆ³ã€è¡›æ˜ŸIDã€åœ°é¢ç«™ID

2. **éæ¿¾åŠŸèƒ½å®Œå–„**
   - æ”¯æ´æŒ‰è¡›æ˜Ÿã€åœ°é¢ç«™ã€æœ€å°æŒçºŒæ™‚é–“éæ¿¾
   - FIFO é…å°é‚è¼¯è™•ç† enter/exit äº‹ä»¶

3. **éŒ¯èª¤å®¹å¿æ€§**
   - `errors="ignore"` è™•ç†ç·¨ç¢¼å•é¡Œ
   - å„ªé›…è™•ç†ç¼ºå¤±çš„ exit äº‹ä»¶

#### âŒ ç™¼ç¾å•é¡Œ

**P1 - é«˜å„ªå…ˆç´š: æ™‚å€è™•ç†ä¸å®Œæ•´**
```python
# âŒ ç•¶å‰ä»£ç¢¼ (ç¬¬ 32 è¡Œ)
ap.add_argument("--tz", default="UTC")  # åƒæ•¸å®šç¾©ä½†æœªä½¿ç”¨

# âœ… å»ºè­°ä¿®å¾©
def parse_dt(s: str, tz: str = "UTC") -> datetime:
    """Parse timestamp with configurable timezone."""
    dt = datetime.strptime(s, "%Y-%m-%dT%H:%M:%SZ")
    if tz == "UTC":
        return dt.replace(tzinfo=timezone.utc)
    else:
        import pytz
        return dt.replace(tzinfo=pytz.timezone(tz))
```

**P2 - ä¸­å„ªå…ˆç´š: ç¼ºå°‘é¡å‹æç¤º**
```python
# âŒ ç•¶å‰ä»£ç¢¼ (ç¬¬ 76-80 è¡Œ)
def dur(w):  # ç¼ºå°‘é¡å‹æç¤º
    if not w.get("start") or not w.get("end"): return 0
    s = parse_dt(w["start"]) ; e = parse_dt(w["end"])
    return max(0, int((e-s).total_seconds()))

# âœ… å»ºè­°æ”¹é€²
def calculate_window_duration(window: Dict[str, Any]) -> int:
    """Calculate window duration in seconds.

    Args:
        window: Window dictionary with 'start' and 'end' timestamps

    Returns:
        Duration in seconds, or 0 if timestamps are missing
    """
    if not window.get("start") or not window.get("end"):
        return 0
    start_time = parse_dt(window["start"])
    end_time = parse_dt(window["end"])
    return max(0, int((end_time - start_time).total_seconds()))
```

**P3 - ä½å„ªå…ˆç´š: é…å°é‚è¼¯å¯å„ªåŒ–**
```python
# âŒ ç•¶å‰ä»£ç¢¼ (ç¬¬ 52-64 è¡Œ) - O(nÂ²) æ™‚é–“è¤‡é›œåº¦
for i, w in enters:
    for j, x in exits:
        if j in used_exits:
            continue
        if x["sat"]==w["sat"] and x["gw"]==w["gw"]:
            paired.append(...)
            used_exits.add(j)
            break

# âœ… å»ºè­°å„ªåŒ– - O(n) æ™‚é–“è¤‡é›œåº¦
from collections import defaultdict

def pair_command_windows(enters: List, exits: List) -> List[Dict]:
    """Pair enter/exit events using optimized algorithm."""
    exit_queues = defaultdict(list)

    # Group exits by (sat, gw)
    for idx, window in exits:
        key = (window["sat"], window["gw"])
        exit_queues[key].append((idx, window))

    paired = []
    for _, enter_window in enters:
        key = (enter_window["sat"], enter_window["gw"])
        if key in exit_queues and exit_queues[key]:
            _, exit_window = exit_queues[key].pop(0)
            paired.append({
                "type": "cmd",
                "start": enter_window["start"],
                "end": exit_window["end"],
                "sat": enter_window["sat"],
                "gw": enter_window["gw"],
                "source": "log"
            })

    return paired
```

#### ğŸ“ æ”¹é€²å»ºè­°

1. **æ·»åŠ è¼¸å…¥é©—è­‰**
   ```python
   def validate_timestamp_format(ts: str) -> bool:
       """Validate timestamp matches expected format."""
       pattern = r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}Z$'
       return bool(re.match(pattern, ts))
   ```

2. **æ·»åŠ æ—¥èªŒè¨˜éŒ„**
   ```python
   import logging

   logger = logging.getLogger(__name__)

   def main():
       # ...
       logger.info(f"Processing log file: {args.log}")
       logger.info(f"Found {len(final)} windows after filtering")
   ```

3. **æ”¯æ´æ‰¹æ¬¡è™•ç†**
   ```python
   ap.add_argument("--batch", nargs='+', type=Path,
                   help="Process multiple log files")
   ```

---

### 2. `scripts/gen_scenario.py` - NS-3 å ´æ™¯ç”Ÿæˆå™¨

**è©•åˆ†**: 8.5/10

#### âœ… å„ªé»
1. **æ¸…æ™°çš„ OOP è¨­è¨ˆ**: `ScenarioGenerator` é¡å°è£è‰¯å¥½
2. **æ¨¡å¼æ”¯æ´**: Transparent vs. Regenerative æ¨¡å¼åˆ‡æ›
3. **å®Œæ•´çš„æ‹“æ’²ç”Ÿæˆ**: è¡›æ˜Ÿã€åœ°é¢ç«™ã€éˆè·¯
4. **äº‹ä»¶é©…å‹•**: ç”Ÿæˆ link_up/link_down äº‹ä»¶åºåˆ—

#### âŒ ç™¼ç¾å•é¡Œ

**P1 - é«˜å„ªå…ˆç´š: ç¡¬ç·¨ç¢¼å¸¸æ•¸**
```python
# âŒ ç•¶å‰ä»£ç¢¼ (ç¬¬ 59-60 è¡Œ)
'altitude_km': 550,  # Default - ç¡¬ç·¨ç¢¼

# âœ… å»ºè­°æ”¹é€²
@dataclass
class SatelliteConfig:
    """Satellite configuration."""
    orbit_type: str = "LEO"
    altitude_km: float = 550
    inclination_deg: float = 53.0

class ScenarioGenerator:
    def __init__(self, mode: str = "transparent",
                 sat_config: Optional[SatelliteConfig] = None):
        self.sat_config = sat_config or SatelliteConfig()
```

**P2 - ä¸­å„ªå…ˆç´š: å»¶é²è¨ˆç®—éæ–¼ç°¡åŒ–**
```python
# âŒ ç•¶å‰ä»£ç¢¼ (ç¬¬ 119-124 è¡Œ)
def _compute_base_latency(self) -> float:
    if self.mode == 'transparent':
        return 5.0  # ms - é­”è¡“æ•¸å­—
    else:
        return 10.0  # ms - é­”è¡“æ•¸å­—

# âœ… å»ºè­°æ”¹é€²
def _compute_base_latency(self, altitude_km: float = 550) -> float:
    """Compute realistic propagation delay.

    Based on physics: delay = (2 * distance) / speed_of_light
    """
    SPEED_OF_LIGHT_KM_MS = 299792.458  # km/ms

    # Two-way propagation (up + down)
    propagation_delay_ms = (2 * altitude_km) / SPEED_OF_LIGHT_KM_MS

    # Add processing delay for regenerative mode
    processing_delay_ms = 5.0 if self.mode == 'regenerative' else 0.0

    return propagation_delay_ms + processing_delay_ms
```

**P3 - ä½å„ªå…ˆç´š: NS-3 å°å‡ºä»£ç¢¼æœªæ¸¬è©¦**
```python
# âŒ ç•¶å‰ä»£ç¢¼ (ç¬¬ 138-189 è¡Œ) - ç”Ÿæˆçš„ NS-3 ä»£ç¢¼èªæ³•å¯èƒ½æœ‰èª¤
# ç¼ºå°‘å°æ‡‰çš„æ¸¬è©¦æ¡ˆä¾‹

# âœ… å»ºè­°æ·»åŠ æ¸¬è©¦
def test_export_ns3_syntax(self, sample_scenario):
    """Verify exported NS-3 code has valid Python syntax."""
    generator = ScenarioGenerator()
    ns3_script = generator.export_ns3(sample_scenario)

    # Verify syntax
    try:
        compile(ns3_script, '<string>', 'exec')
    except SyntaxError as e:
        pytest.fail(f"Generated NS-3 code has syntax error: {e}")
```

#### ğŸ“ æ”¹é€²å»ºè­°

1. **æ”¯æ´å¤šæ˜Ÿç³»é…ç½®**
   ```python
   class ConstellationConfig:
       """Multi-constellation configuration."""
       def __init__(self):
           self.constellations = {
               'starlink': {'count': 550, 'altitude_km': 550, 'planes': 72},
               'oneweb': {'count': 648, 'altitude_km': 1200, 'planes': 18},
               'kuiper': {'count': 3236, 'altitude_km': 630, 'planes': 34}
           }
   ```

2. **æ·»åŠ æ‹“æ’²é©—è­‰**
   ```python
   def validate_topology(self, topology: Dict) -> Tuple[bool, List[str]]:
       """Validate generated topology for consistency."""
       errors = []

       if not topology['satellites']:
           errors.append("No satellites in topology")

       if not topology['gateways']:
           errors.append("No gateways in topology")

       # Verify all links reference valid nodes
       sat_ids = {s['id'] for s in topology['satellites']}
       gw_ids = {g['id'] for g in topology['gateways']}

       for link in topology['links']:
           if link['source'] not in sat_ids:
               errors.append(f"Invalid source: {link['source']}")
           if link['target'] not in gw_ids:
               errors.append(f"Invalid target: {link['target']}")

       return len(errors) == 0, errors
   ```

---

### 3. `scripts/metrics.py` - KPI è¨ˆç®—å™¨

**è©•åˆ†**: 9.0/10 â­ **æœ€ä½³æ¨¡çµ„**

#### âœ… å„ªé»
1. **ç‰©ç†å…¬å¼æ­£ç¢º**: ä½¿ç”¨å…‰é€Ÿå¸¸æ•¸é€²è¡Œå‚³æ’­å»¶é²è¨ˆç®—
2. **å®Œæ•´çš„æŒ‡æ¨™**: å»¶é²ã€ååé‡ã€åˆ©ç”¨ç‡
3. **çµ±è¨ˆåŠŸèƒ½**: å¹³å‡å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼ã€P95
4. **å¤šæ ¼å¼è¼¸å‡º**: JSON æ‘˜è¦ + CSV è©³ç´°æ•¸æ“š

#### âŒ ç™¼ç¾å•é¡Œ

**P1 - ä¸­å„ªå…ˆç´š: æ’éšŠå»¶é²æ¨¡å‹éæ–¼ç°¡åŒ–**
```python
# âŒ ç•¶å‰ä»£ç¢¼ (ç¬¬ 111-119 è¡Œ)
def _estimate_queuing_delay(self, duration_sec: float) -> float:
    if duration_sec < 60:
        return 0.5  # ç¡¬ç·¨ç¢¼é–¾å€¼
    elif duration_sec < 300:
        return 2.0
    else:
        return 5.0

# âœ… å»ºè­°æ”¹é€² - ä½¿ç”¨ M/M/1 æ’éšŠæ¨¡å‹
def _estimate_queuing_delay(
    self,
    duration_sec: float,
    arrival_rate: float = 100,  # packets/sec
    service_rate: float = 120   # packets/sec
) -> float:
    """Estimate queuing delay using M/M/1 model.

    Args:
        duration_sec: Session duration
        arrival_rate: Packet arrival rate (Î»)
        service_rate: Service rate (Î¼)

    Returns:
        Average queuing delay in milliseconds
    """
    if arrival_rate >= service_rate:
        # Unstable queue - use worst case
        return duration_sec * 10.0

    # M/M/1 formula: W = 1 / (Î¼ - Î»)
    utilization = arrival_rate / service_rate
    avg_wait_sec = 1 / (service_rate - arrival_rate)

    # Convert to milliseconds
    return avg_wait_sec * 1000
```

**P2 - ä½å„ªå…ˆç´š: ååé‡è¨ˆç®—å‡è¨­å›ºå®šåˆ©ç”¨ç‡**
```python
# âŒ ç•¶å‰ä»£ç¢¼ (ç¬¬ 128-131 è¡Œ)
def _compute_throughput(self, duration_sec: float, data_rate_mbps: float) -> float:
    return data_rate_mbps * 0.8  # å›ºå®š 80% åˆ©ç”¨ç‡

# âœ… å»ºè­°æ”¹é€² - å‹•æ…‹åˆ©ç”¨ç‡æ¨¡å‹
def _compute_throughput(
    self,
    duration_sec: float,
    data_rate_mbps: float,
    congestion_level: str = "low"
) -> float:
    """Compute throughput with dynamic utilization."""
    utilization_map = {
        "low": 0.90,      # < 1 min sessions
        "medium": 0.75,   # 1-5 min sessions
        "high": 0.60      # > 5 min sessions
    }

    if duration_sec < 60:
        level = "low"
    elif duration_sec < 300:
        level = "medium"
    else:
        level = "high"

    utilization = utilization_map.get(congestion_level, 0.80)
    return data_rate_mbps * utilization
```

#### ğŸ“ æ”¹é€²å»ºè­°

1. **æ·»åŠ æ›´å¤š KPI**
   ```python
   def compute_additional_kpis(self, metrics: List[Dict]) -> Dict:
       """Compute advanced KPIs."""
       return {
           'jitter_ms': self._compute_jitter(metrics),
           'packet_loss_percent': self._estimate_packet_loss(metrics),
           'availability_percent': self._compute_availability(metrics),
           'handover_rate': self._compute_handover_rate(metrics)
       }
   ```

2. **æ”¯æ´æ™‚é–“åºåˆ—åˆ†æ**
   ```python
   def analyze_time_series(self, metrics: List[Dict], window_minutes: int = 60):
       """Analyze metrics over time windows."""
       import pandas as pd

       df = pd.DataFrame(metrics)
       df['timestamp'] = pd.to_datetime(df['start'])
       df.set_index('timestamp', inplace=True)

       # Resample to time windows
       resampled = df.resample(f'{window_minutes}T').agg({
           'latency.total_ms': ['mean', 'std', 'max'],
           'throughput.average_mbps': ['mean', 'min', 'max']
       })

       return resampled
   ```

---

### 4. `scripts/scheduler.py` - æ³¢æŸæ’ç¨‹å™¨

**è©•åˆ†**: 7.5/10

#### âœ… å„ªé»
1. **è²ªå©ªæ¼”ç®—æ³•**: ç°¡å–®æœ‰æ•ˆçš„æ’ç¨‹ç­–ç•¥
2. **è¡çªæª¢æ¸¬**: æº–ç¢ºæª¢æ¸¬æ™‚é–“é‡ç–Š
3. **å®¹é‡ç®¡ç†**: æ”¯æ´æ¯å€‹åœ°é¢ç«™çš„æ³¢æŸå®¹é‡é™åˆ¶
4. **Dataclass ä½¿ç”¨**: æ¸…æ™°çš„ `TimeSlot` æ•¸æ“šçµæ§‹

#### âŒ ç™¼ç¾å•é¡Œ

**P1 - é«˜å„ªå…ˆç´š: è²ªå©ªæ¼”ç®—æ³•éæœ€å„ªè§£**
```python
# âŒ ç•¶å‰ä»£ç¢¼ (ç¬¬ 43-56 è¡Œ) - ç°¡å–®è²ªå©ªå¯èƒ½éŒ¯éæœ€å„ªæ’ç¨‹
for slot in slots:
    if self._can_assign(slot):
        slot.assigned = True
        self.schedule.append(slot)

# âœ… å»ºè­°æ”¹é€² - ä½¿ç”¨ OR-Tools é€²è¡Œå„ªåŒ–æ’ç¨‹
from ortools.sat.python import cp_model

def optimize_schedule(self, slots: List[TimeSlot]) -> List[TimeSlot]:
    """Optimize schedule using constraint programming."""
    model = cp_model.CpModel()

    # Variables: binary decision for each slot
    slot_vars = [
        model.NewBoolVar(f'slot_{i}')
        for i in range(len(slots))
    ]

    # Constraints: no overlapping slots on same gateway
    for i, slot1 in enumerate(slots):
        for j, slot2 in enumerate(slots):
            if i >= j:
                continue

            if slot1.gateway == slot2.gateway and self._overlaps(slot1, slot2):
                # At most one can be assigned
                model.Add(slot_vars[i] + slot_vars[j] <= 1)

    # Objective: maximize assigned slots
    model.Maximize(sum(slot_vars))

    # Solve
    solver = cp_model.CpSolver()
    status = solver.Solve(model)

    if status in [cp_model.OPTIMAL, cp_model.FEASIBLE]:
        assigned = [
            slot for i, slot in enumerate(slots)
            if solver.Value(slot_vars[i]) == 1
        ]
        return assigned
    else:
        return []  # Fallback to greedy
```

**P2 - ä¸­å„ªå…ˆç´š: ç¼ºå°‘å„ªå…ˆç´šæ’ç¨‹**
```python
# ç•¶å‰ç„¡å„ªå…ˆç´šæ¦‚å¿µ

# âœ… å»ºè­°æ·»åŠ å„ªå…ˆç´šæ”¯æ´
@dataclass
class TimeSlot:
    start: datetime
    end: datetime
    satellite: str
    gateway: str
    window_type: str
    priority: int = 0  # æ–°å¢: 0=ä½, 1=ä¸­, 2=é«˜
    assigned: bool = False

def schedule_with_priority(self, slots: List[TimeSlot]) -> List[TimeSlot]:
    """Schedule with priority-based preemption."""
    # Sort by priority (descending) then start time
    slots.sort(key=lambda s: (-s.priority, s.start))

    for slot in slots:
        if self._can_assign(slot):
            slot.assigned = True
            self.schedule.append(slot)
        elif slot.priority >= 2:  # High priority
            # Attempt preemption
            if self._try_preempt(slot):
                slot.assigned = True
                self.schedule.append(slot)
```

**P3 - ä½å„ªå…ˆç´š: çµ±è¨ˆè¨ˆç®—å¯æ“´å±•**
```python
# âœ… å»ºè­°æ·»åŠ æ›´å¤šçµ±è¨ˆæŒ‡æ¨™
def compute_statistics(self) -> Dict:
    """Compute comprehensive scheduling statistics."""
    stats = {
        'basic': self._compute_basic_stats(),
        'gateway_stats': self._compute_gateway_stats(),
        'satellite_stats': self._compute_satellite_stats(),
        'temporal_stats': self._compute_temporal_stats()
    }
    return stats

def _compute_gateway_stats(self) -> Dict:
    """Per-gateway utilization statistics."""
    gateway_stats = {}

    for gw in set(slot.gateway for slot in self.schedule):
        gw_slots = [s for s in self.schedule if s.gateway == gw]
        total_duration = sum(
            (s.end - s.start).total_seconds() for s in gw_slots
        )

        gateway_stats[gw] = {
            'slots_assigned': len(gw_slots),
            'total_duration_sec': total_duration,
            'average_duration_sec': total_duration / len(gw_slots) if gw_slots else 0,
            'utilization_percent': (total_duration / 86400) * 100  # Assuming 24h window
        }

    return gateway_stats
```

#### ğŸ“ æ”¹é€²å»ºè­°

1. **æ”¯æ´å‹•æ…‹é‡æ’ç¨‹**
   ```python
   def reschedule_on_failure(self, failed_slot: TimeSlot) -> Optional[TimeSlot]:
       """Attempt to reschedule failed slot."""
       # Find alternative time window
       for slot in self.conflicts:
           if slot.satellite == failed_slot.satellite:
               # Try shifting time
               shifted = self._try_shift_slot(slot)
               if shifted and self._can_assign(shifted):
                   return shifted
       return None
   ```

2. **æ·»åŠ å¯è¦–åŒ–å°å‡º**
   ```python
   def export_gantt_data(self) -> Dict:
       """Export data for Gantt chart visualization."""
       return {
           'tasks': [
               {
                   'id': f"{s.satellite}-{s.gateway}",
                   'start': s.start.isoformat(),
                   'end': s.end.isoformat(),
                   'resource': s.gateway,
                   'status': 'assigned' if s.assigned else 'conflict'
               }
               for s in (self.schedule + self.conflicts)
           ]
       }
   ```

---

### 5. `tests/test_parser.py` - è§£æå™¨æ¸¬è©¦

**è©•åˆ†**: 9.5/10 â­ **æ¸¬è©¦å…¸ç¯„**

#### âœ… å„ªé»
1. **å®Œæ•´çš„æ¸¬è©¦è¦†è“‹**:
   - å–®å…ƒæ¸¬è©¦ (æ™‚é–“æˆ³ã€æ­£å‰‡)
   - æ•´åˆæ¸¬è©¦ (å®Œæ•´è§£ææµç¨‹)
   - é‚Šç•Œæ¢ä»¶ (ç©ºæª”æ¡ˆã€ç¼ºå¤±äº‹ä»¶)
   - æ•ˆèƒ½æ¸¬è©¦ (å¤§è¦æ¨¡æ•¸æ“š)

2. **å„ªç§€çš„æ¸¬è©¦çµ„ç¹”**:
   - æŒ‰åŠŸèƒ½åˆ†é¡ (`TestTimestampParsing`, `TestRegexPatterns`, ...)
   - æ¸…æ™°çš„æ¸¬è©¦åç¨±
   - è±å¯Œçš„ docstrings

3. **Pytest æœ€ä½³å¯¦è¸**:
   - ä½¿ç”¨ fixtures (`temp_log_file`, `temp_output_file`)
   - Parametrize æ¸¬è©¦ (å¯æ“´å±•)
   - Benchmark æ•´åˆ

#### âŒ ç™¼ç¾å•é¡Œ

**P1 - ä½å„ªå…ˆç´š: ç¼ºå°‘åƒæ•¸åŒ–æ¸¬è©¦**
```python
# âœ… å»ºè­°æ·»åŠ åƒæ•¸åŒ–æ¸¬è©¦ä»¥æé«˜è¦†è“‹ç‡
@pytest.mark.parametrize("timestamp,expected_valid", [
    ("2025-01-08T10:15:30Z", True),
    ("2025-13-01T10:15:30Z", False),  # Invalid month
    ("2025-01-32T10:15:30Z", False),  # Invalid day
    ("2025-01-08T25:15:30Z", False),  # Invalid hour
])
def test_parse_timestamp_validation(timestamp, expected_valid):
    """Test timestamp validation with various inputs."""
    if expected_valid:
        result = parse_dt(timestamp)
        assert isinstance(result, datetime)
    else:
        with pytest.raises(ValueError):
            parse_dt(timestamp)
```

**P2 - ä½å„ªå…ˆç´š: æ•ˆèƒ½æ¸¬è©¦é–¾å€¼æœªå®šç¾©**
```python
# âŒ ç•¶å‰ä»£ç¢¼ (ç¬¬ 388 è¡Œ)
result = benchmark(parse)  # ç„¡æ˜ç¢ºæ•ˆèƒ½è¦æ±‚

# âœ… å»ºè­°æ·»åŠ æ•ˆèƒ½æ–·è¨€
def test_parse_large_log_performance(self, tmp_path: Path, benchmark):
    """Test parsing performance meets SLA."""
    # ... setup code ...

    result = benchmark(parse)

    # Performance assertions
    assert result.stats['mean'] < 1.0, "Parsing should complete within 1 second"
    assert result.stats['max'] < 2.0, "Max time should be under 2 seconds"
```

#### ğŸ“ æ”¹é€²å»ºè­°

1. **æ·»åŠ æ•´åˆæ¸¬è©¦**
   ```python
   def test_full_pipeline_integration(self):
       """Test complete pipeline: parse -> scenario -> metrics -> schedule."""
       # Parse
       parse_result = parse_oasis_log(test_log)

       # Generate scenario
       scenario = gen_scenario(parse_result)

       # Compute metrics
       metrics = compute_metrics(scenario)

       # Schedule
       schedule = schedule_beams(scenario)

       # Verify end-to-end consistency
       assert len(metrics) == len(schedule.schedule)
   ```

2. **æ·»åŠ å±¬æ€§æ¸¬è©¦ (Hypothesis)**
   ```python
   from hypothesis import given, strategies as st

   @given(st.datetimes(min_value=datetime(2025,1,1), max_value=datetime(2030,12,31)))
   def test_parse_dt_roundtrip(dt):
       """Property test: parse(format(dt)) == dt."""
       formatted = dt.strftime("%Y-%m-%dT%H:%M:%SZ")
       parsed = parse_dt(formatted)
       assert parsed.replace(microsecond=0) == dt.replace(microsecond=0, tzinfo=timezone.utc)
   ```

---

## ğŸ”’ å®‰å…¨æ€§å¯©æŸ¥

### ç™¼ç¾çš„å®‰å…¨å•é¡Œ

#### 1. è·¯å¾‘æ³¨å…¥é¢¨éšª (ä¸­é¢¨éšª)
```python
# âŒ parse_oasis_log.py (ç¬¬ 39 è¡Œ)
with args.log.open("r", encoding="utf-8", errors="ignore") as f:
    content = f.read()  # è®€å–æ•´å€‹æª”æ¡ˆåˆ°è¨˜æ†¶é«”

# âœ… å»ºè­°ä¿®å¾©
import os

def safe_read_file(file_path: Path, max_size_mb: int = 100) -> str:
    """Safely read file with size limit."""
    file_size = os.path.getsize(file_path)
    max_bytes = max_size_mb * 1024 * 1024

    if file_size > max_bytes:
        raise ValueError(f"File too large: {file_size} bytes (max {max_bytes})")

    with file_path.open("r", encoding="utf-8", errors="ignore") as f:
        return f.read()
```

#### 2. JSON ååºåˆ—åŒ–é¢¨éšª (ä½é¢¨éšª)
```python
# âœ… å»ºè­°æ·»åŠ  Schema é©—è­‰
from jsonschema import validate, ValidationError

WINDOWS_SCHEMA = {
    "type": "object",
    "required": ["meta", "windows"],
    "properties": {
        "meta": {
            "type": "object",
            "required": ["source", "count"]
        },
        "windows": {
            "type": "array",
            "items": {
                "type": "object",
                "required": ["type", "sat", "gw"]
            }
        }
    }
}

def load_and_validate_windows(path: Path) -> Dict:
    """Load and validate windows JSON."""
    with path.open() as f:
        data = json.load(f)

    try:
        validate(instance=data, schema=WINDOWS_SCHEMA)
    except ValidationError as e:
        raise ValueError(f"Invalid windows data: {e.message}")

    return data
```

#### 3. è³‡æºæ´©æ¼ (ä½é¢¨éšª)
```python
# âœ… å»ºè­°ä½¿ç”¨ context manager
class MetricsCalculator:
    def export_csv(self, output_path: Path):
        # âœ… ç•¶å‰å·²æ­£ç¢ºä½¿ç”¨ with
        with output_path.open('w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            # ...
```

---

## âš¡ æ•ˆèƒ½å„ªåŒ–å»ºè­°

### 1. å¤§è¦æ¨¡æ•¸æ“šè™•ç†å„ªåŒ–

**ç•¶å‰ç“¶é ¸**: 100,000+ è¦–çª—æ™‚è¨˜æ†¶é«”ä½¿ç”¨éé«˜

**å„ªåŒ–æ–¹æ¡ˆ**: æµå¼è™•ç†
```python
def parse_large_log_streaming(log_path: Path, chunk_size: int = 1000):
    """Parse large log files using streaming approach."""
    import itertools

    def window_generator():
        with log_path.open("r") as f:
            for line in f:
                # Process line-by-line
                for pattern in [PAT_ENTER, PAT_EXIT, PAT_XBAND]:
                    match = pattern.search(line)
                    if match:
                        yield process_match(match)

    # Process in chunks
    for chunk in itertools.islice(window_generator(), chunk_size):
        yield chunk
```

### 2. ä¸¦è¡Œè™•ç†

**å»ºè­°**: ä½¿ç”¨ multiprocessing è™•ç†å¤šå€‹æ—¥èªŒæª”æ¡ˆ
```python
from multiprocessing import Pool
from functools import partial

def parse_log_parallel(log_files: List[Path], num_workers: int = 4) -> List[Dict]:
    """Parse multiple log files in parallel."""
    with Pool(processes=num_workers) as pool:
        results = pool.map(parse_single_log, log_files)

    # Merge results
    combined = {
        "meta": {"source": "batch", "count": sum(r["meta"]["count"] for r in results)},
        "windows": [w for r in results for w in r["windows"]]
    }

    return combined
```

### 3. å¿«å–æ©Ÿåˆ¶

**å»ºè­°**: å¿«å–æ˜‚è²´çš„è¨ˆç®—çµæœ
```python
from functools import lru_cache

class MetricsCalculator:
    @lru_cache(maxsize=1000)
    def _compute_propagation_delay(self, altitude_km: float = 550) -> float:
        """Cached propagation delay computation."""
        distance_km = altitude_km * 2
        delay_ms = (distance_km / self.SPEED_OF_LIGHT) * 1000
        return delay_ms
```

---

## ğŸ“ æ¶æ§‹æ”¹é€²å»ºè­°

### 1. æ·»åŠ é…ç½®ç®¡ç†å±¤

**ç•¶å‰å•é¡Œ**: ç¡¬ç·¨ç¢¼å¸¸æ•¸æ•£è½å„è™•

**å»ºè­°æ–¹æ¡ˆ**: é›†ä¸­é…ç½®ç®¡ç†
```python
# config/settings.py
from dataclasses import dataclass
from typing import Dict

@dataclass
class PhysicsConstants:
    """Physical constants for satellite communications."""
    SPEED_OF_LIGHT_KM_S: float = 299792.458
    EARTH_RADIUS_KM: float = 6371.0

@dataclass
class SatelliteDefaults:
    """Default satellite parameters."""
    LEO_ALTITUDE_KM: float = 550
    MEO_ALTITUDE_KM: float = 8000
    GEO_ALTITUDE_KM: float = 35786

@dataclass
class NetworkDefaults:
    """Default network parameters."""
    DATA_RATE_MBPS: float = 50
    MTU_BYTES: int = 1500
    BUFFER_SIZE_MB: int = 10

class Config:
    """Centralized configuration."""
    physics = PhysicsConstants()
    satellite = SatelliteDefaults()
    network = NetworkDefaults()

# Usage
from config.settings import Config
delay = (2 * Config.satellite.LEO_ALTITUDE_KM) / Config.physics.SPEED_OF_LIGHT_KM_S
```

### 2. æ·»åŠ æŠ½è±¡å±¤

**å»ºè­°**: å®šç¾©æ¸…æ™°çš„ä»‹é¢
```python
# interfaces/parser.py
from abc import ABC, abstractmethod

class ILogParser(ABC):
    """Interface for log parsers."""

    @abstractmethod
    def parse(self, log_path: Path) -> Dict:
        """Parse log file and return windows."""
        pass

    @abstractmethod
    def validate(self, data: Dict) -> bool:
        """Validate parsed data."""
        pass

class OASISParser(ILogParser):
    """OASIS-specific parser implementation."""

    def parse(self, log_path: Path) -> Dict:
        # Current implementation
        pass

class StarlinkParser(ILogParser):
    """Starlink-specific parser (future)."""

    def parse(self, log_path: Path) -> Dict:
        # TBD
        pass
```

### 3. æ·»åŠ ç®¡ç·šç·¨æ’å™¨

**å»ºè­°**: çµ±ä¸€ç®¡ç·šåŸ·è¡Œé‚è¼¯
```python
# pipeline/orchestrator.py
from typing import List, Callable
from dataclasses import dataclass

@dataclass
class PipelineStep:
    """Single pipeline step."""
    name: str
    function: Callable
    inputs: List[str]
    outputs: List[str]

class PipelineOrchestrator:
    """Orchestrate multi-step pipeline execution."""

    def __init__(self):
        self.steps: List[PipelineStep] = []
        self.artifacts: Dict[str, Any] = {}

    def add_step(self, step: PipelineStep):
        """Add step to pipeline."""
        self.steps.append(step)

    def execute(self, initial_inputs: Dict):
        """Execute all pipeline steps."""
        self.artifacts.update(initial_inputs)

        for step in self.steps:
            # Gather inputs
            inputs = {k: self.artifacts[k] for k in step.inputs}

            # Execute step
            result = step.function(**inputs)

            # Store outputs
            if isinstance(result, dict):
                self.artifacts.update(result)
            else:
                self.artifacts[step.outputs[0]] = result

        return self.artifacts

# Usage
pipeline = PipelineOrchestrator()

pipeline.add_step(PipelineStep(
    name="parse",
    function=parse_oasis_log,
    inputs=["log_file"],
    outputs=["windows"]
))

pipeline.add_step(PipelineStep(
    name="scenario",
    function=generate_scenario,
    inputs=["windows"],
    outputs=["scenario"]
))

results = pipeline.execute({"log_file": "data/sample.log"})
```

---

## ğŸš€ æ“´å±•åŠŸèƒ½å»ºè­°

### æ–°åŠŸèƒ½ 1: Starlink æ‰¹æ¬¡è™•ç†å™¨

**æ–‡ä»¶**: `scripts/starlink_batch_processor.py`

```python
#!/usr/bin/env python3
"""Starlink constellation batch processor.

Features:
- Process multiple Starlink TLE files
- Generate constellation-wide coverage analysis
- Batch compute visibility windows for all satellites
"""
from __future__ import annotations
import json
from pathlib import Path
from typing import List, Dict
from dataclasses import dataclass
from sgp4.api import Satellite, jday
from datetime import datetime, timedelta

@dataclass
class StarlinkSatellite:
    """Starlink satellite data."""
    norad_id: str
    name: str
    tle_line1: str
    tle_line2: str
    orbit_plane: int
    orbit_position: int

class StarlinkBatchProcessor:
    """Batch process Starlink constellation."""

    def __init__(self):
        self.satellites: List[StarlinkSatellite] = []

    def load_tle_batch(self, tle_dir: Path) -> int:
        """Load all TLE files from directory."""
        count = 0
        for tle_file in tle_dir.glob("*.tle"):
            count += self._load_tle_file(tle_file)
        return count

    def _load_tle_file(self, tle_path: Path) -> int:
        """Load single TLE file."""
        # Implementation TBD
        pass

    def compute_coverage_map(
        self,
        ground_stations: List[Dict],
        start_time: datetime,
        duration_hours: int,
        time_step_minutes: int = 1
    ) -> Dict:
        """Compute coverage map for all ground stations."""
        # Implementation TBD
        pass

    def generate_batch_report(self, output_dir: Path):
        """Generate comprehensive batch report."""
        # Implementation TBD
        pass
```

**æ¸¬è©¦**: `tests/test_starlink_batch.py`

```python
def test_starlink_batch_load_tle():
    """Test loading multiple Starlink TLE files."""
    processor = StarlinkBatchProcessor()
    count = processor.load_tle_batch(Path("data/starlink_tle/"))
    assert count > 0

def test_starlink_coverage_computation():
    """Test coverage map generation."""
    processor = StarlinkBatchProcessor()
    # ... setup ...
    coverage = processor.compute_coverage_map(ground_stations, start, 24)
    assert 'coverage_percent' in coverage
```

### æ–°åŠŸèƒ½ 2: å¤šæ˜Ÿç³»æ”¯æ´

**æ–‡ä»¶**: `scripts/multi_constellation.py`

```python
#!/usr/bin/env python3
"""Multi-constellation simulation support.

Supported constellations:
- Starlink (SpaceX)
- OneWeb
- Kuiper (Amazon)
- Custom constellations
"""

class ConstellationManager:
    """Manage multiple satellite constellations."""

    def __init__(self):
        self.constellations: Dict[str, List[Satellite]] = {}

    def add_constellation(self, name: str, satellites: List):
        """Add constellation to manager."""
        self.constellations[name] = satellites

    def compute_inter_constellation_handover(self) -> Dict:
        """Compute handover between different constellations."""
        # Implementation TBD
        pass

    def optimize_multi_constellation_routing(self) -> Dict:
        """Optimize routing across multiple constellations."""
        # Implementation TBD
        pass
```

### æ–°åŠŸèƒ½ 3: è¦–è¦ºåŒ–æ¨¡çµ„

**æ–‡ä»¶**: `scripts/visualizer.py`

```python
#!/usr/bin/env python3
"""Visualization module for satellite communications."""
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.animation import FuncAnimation
import numpy as np

class SatelliteVisualizer:
    """Visualize satellite network topology and metrics."""

    def plot_coverage_map(self, coverage_data: Dict, output_path: Path):
        """Plot ground coverage map."""
        fig, ax = plt.subplots(figsize=(12, 8))
        # Implementation TBD
        pass

    def plot_latency_heatmap(self, metrics: List[Dict], output_path: Path):
        """Plot latency heatmap over time and geography."""
        # Implementation TBD
        pass

    def plot_beam_schedule_gantt(self, schedule: List, output_path: Path):
        """Plot Gantt chart of beam scheduling."""
        # Implementation TBD
        pass

    def animate_satellite_passes(
        self,
        satellite_positions: List,
        ground_stations: List,
        output_path: Path
    ):
        """Create animation of satellite passes."""
        # Implementation TBD
        pass
```

**æ¸¬è©¦**: `tests/test_visualization.py`

```python
def test_plot_coverage_map(sample_coverage_data, tmp_path):
    """Test coverage map generation."""
    viz = SatelliteVisualizer()
    output = tmp_path / "coverage.png"
    viz.plot_coverage_map(sample_coverage_data, output)
    assert output.exists()

def test_plot_latency_heatmap(sample_metrics, tmp_path):
    """Test latency heatmap generation."""
    viz = SatelliteVisualizer()
    output = tmp_path / "latency.png"
    viz.plot_latency_heatmap(sample_metrics, output)
    assert output.exists()
```

---

## ğŸ“‹ é‡æ§‹è¨ˆåŠƒ

### ç¬¬ä¸€éšæ®µ: ä»£ç¢¼å“è³ªæå‡ (1-2 é€±)

**å„ªå…ˆç´š P0 (å¿…é ˆ)**:
1. âœ… æ·»åŠ å®Œæ•´çš„é¡å‹æç¤ºåˆ°æ‰€æœ‰å‡½æ•¸
2. âœ… ä¿®å¾© timezone åƒæ•¸ä½¿ç”¨
3. âœ… å„ªåŒ– O(nÂ²) é…å°æ¼”ç®—æ³•
4. âœ… æ·»åŠ è¼¸å…¥é©—è­‰èˆ‡éŒ¯èª¤è™•ç†

**å„ªå…ˆç´š P1 (æ‡‰è©²)**:
5. âœ… å¯¦ç¾é›†ä¸­åŒ–é…ç½®ç®¡ç†
6. âœ… æ·»åŠ æ—¥èªŒè¨˜éŒ„
7. âœ… æ”¹é€²éŒ¯èª¤è¨Šæ¯

### ç¬¬äºŒéšæ®µ: åŠŸèƒ½æ“´å±• (2-3 é€±)

**æ–°åŠŸèƒ½**:
1. âœ… Starlink æ‰¹æ¬¡è™•ç†å™¨
2. âœ… å¤šæ˜Ÿç³»æ”¯æ´
3. âœ… è¦–è¦ºåŒ–æ¨¡çµ„
4. âœ… é€²éšæ’ç¨‹æ¼”ç®—æ³• (OR-Tools)

**æ¸¬è©¦**:
5. âœ… ç‚ºæ–°åŠŸèƒ½æ·»åŠ å®Œæ•´æ¸¬è©¦
6. âœ… ä¿æŒæ¸¬è©¦è¦†è“‹ç‡ â‰¥ 90%

### ç¬¬ä¸‰éšæ®µ: æ•ˆèƒ½èˆ‡æ¶æ§‹å„ªåŒ– (2-3 é€±)

**æ•ˆèƒ½**:
1. âœ… å¯¦ç¾æµå¼è™•ç†
2. âœ… æ·»åŠ ä¸¦è¡Œè™•ç†
3. âœ… å¯¦ç¾å¿«å–æ©Ÿåˆ¶

**æ¶æ§‹**:
4. âœ… å®šç¾©æ¸…æ™°çš„ä»‹é¢æŠ½è±¡
5. âœ… å¯¦ç¾ç®¡ç·šç·¨æ’å™¨
6. âœ… æ”¹é€²æ¨¡çµ„åŒ–è¨­è¨ˆ

---

## ğŸ¯ æœ€ä½³å¯¦è¸å»ºè­°

### 1. TDD å·¥ä½œæµç¨‹
```bash
# 1. å…ˆå¯«æ¸¬è©¦ (ç´…ç‡ˆ)
pytest tests/test_new_feature.py -v  # Should fail

# 2. å¯¦ç¾åŠŸèƒ½ (ç¶ ç‡ˆ)
# ... write code ...
pytest tests/test_new_feature.py -v  # Should pass

# 3. é‡æ§‹ (é‡æ§‹)
# ... improve code ...
pytest tests/test_new_feature.py -v  # Still passing

# 4. æª¢æŸ¥è¦†è“‹ç‡
pytest --cov=scripts --cov-report=html
```

### 2. ä»£ç¢¼å¯©æŸ¥æª¢æŸ¥æ¸…å–®

âœ… **æäº¤å‰æª¢æŸ¥**:
- [ ] æ‰€æœ‰æ¸¬è©¦é€šé (`pytest tests/ -v`)
- [ ] æ¸¬è©¦è¦†è“‹ç‡ â‰¥ 90% (`pytest --cov=scripts`)
- [ ] Type checking é€šé (`mypy scripts/`)
- [ ] Linting é€šé (`flake8 scripts/`)
- [ ] ä»£ç¢¼æ ¼å¼åŒ– (`black scripts/`)
- [ ] æ–‡æª”æ›´æ–°
- [ ] CHANGELOG æ›´æ–°

### 3. Git Commit è¦ç¯„
```bash
# åŠŸèƒ½
git commit -m "feat(starlink): add batch TLE processor"

# ä¿®å¾©
git commit -m "fix(parser): handle timezone parameter correctly"

# é‡æ§‹
git commit -m "refactor(scheduler): optimize O(nÂ²) pairing algorithm"

# æ¸¬è©¦
git commit -m "test(metrics): add property-based tests with Hypothesis"

# æ–‡æª”
git commit -m "docs(README): update with Starlink batch processing guide"
```

---

## ğŸ“Š ç¸½çµèˆ‡è¡Œå‹•é …ç›®

### ğŸ¯ ç«‹å³è¡Œå‹• (æœ¬é€±)

**å¿…é ˆä¿®å¾© (P0)**:
1. [ ] ä¿®å¾© `parse_oasis_log.py` çš„ timezone åƒæ•¸ä½¿ç”¨
2. [ ] æ·»åŠ è¼¸å…¥é©—è­‰èˆ‡æª”æ¡ˆå¤§å°é™åˆ¶
3. [ ] æ”¹é€² `gen_scenario.py` çš„å»¶é²è¨ˆç®—å…¬å¼
4. [ ] æ·»åŠ  JSON Schema é©—è­‰

**é«˜å„ªå…ˆç´š (P1)**:
5. [ ] å„ªåŒ–é…å°æ¼”ç®—æ³•å¾ O(nÂ²) åˆ° O(n)
6. [ ] å¯¦ç¾é›†ä¸­åŒ–é…ç½®ç®¡ç†
7. [ ] æ·»åŠ å®Œæ•´çš„é¡å‹æç¤º

### ğŸ“ˆ çŸ­æœŸç›®æ¨™ (2 é€±å…§)

**æ–°åŠŸèƒ½é–‹ç™¼**:
1. [ ] å¯¦ç¾ Starlink æ‰¹æ¬¡è™•ç†å™¨ (TDD)
2. [ ] å¯¦ç¾å¤šæ˜Ÿç³»æ”¯æ´æ¨¡çµ„ (TDD)
3. [ ] å¯¦ç¾è¦–è¦ºåŒ–æ¨¡çµ„ (TDD)

**æ¸¬è©¦æ”¹é€²**:
4. [ ] æ·»åŠ åƒæ•¸åŒ–æ¸¬è©¦
5. [ ] æ·»åŠ å±¬æ€§æ¸¬è©¦ (Hypothesis)
6. [ ] æ·»åŠ æ•´åˆæ¸¬è©¦

### ğŸš€ ä¸­æœŸç›®æ¨™ (1 å€‹æœˆå…§)

**æ•ˆèƒ½å„ªåŒ–**:
1. [ ] å¯¦ç¾æµå¼è™•ç†å¤§æª”æ¡ˆ
2. [ ] æ·»åŠ ä¸¦è¡Œè™•ç†æ”¯æ´
3. [ ] å¯¦ç¾å¿«å–æ©Ÿåˆ¶

**æ¶æ§‹æ”¹é€²**:
4. [ ] å®šç¾©æ¸…æ™°çš„ä»‹é¢æŠ½è±¡
5. [ ] å¯¦ç¾ç®¡ç·šç·¨æ’å™¨
6. [ ] æ”¹é€²éŒ¯èª¤è™•ç†èˆ‡æ—¥èªŒ

### ğŸ“š é•·æœŸç›®æ¨™ (3 å€‹æœˆå…§)

1. [ ] å®Œæ•´çš„å¤šæ˜Ÿç³»æ¨¡æ“¬æ”¯æ´
2. [ ] é€²éšæ’ç¨‹æ¼”ç®—æ³• (OR-Tools æ•´åˆ)
3. [ ] å³æ™‚ç›£æ§å„€è¡¨æ¿
4. [ ] CI/CD è‡ªå‹•åŒ–éƒ¨ç½²
5. [ ] æ•ˆèƒ½åŸºæº–æ¸¬è©¦å¥—ä»¶

---

## ğŸ† å°ˆæ¡ˆè©•ä¼°ç¸½çµ

### âœ… å°ˆæ¡ˆå„ªå‹¢
1. **å …å¯¦çš„åŸºç¤**: 98% æ¸¬è©¦è¦†è“‹ç‡ï¼ŒTDD æœ€ä½³å¯¦è¸
2. **æ¸…æ™°çš„æ¶æ§‹**: æ¨¡çµ„åˆ†é›¢è‰¯å¥½ï¼Œè·è²¬æ˜ç¢º
3. **ç”Ÿç”¢å°±ç·’**: Docker + K8s éƒ¨ç½²é©—è­‰å®Œæˆ
4. **å®Œæ•´çš„æ–‡æª”**: READMEã€API æ–‡æª”ã€éƒ¨ç½²æŒ‡å—

### âš ï¸ éœ€è¦æ”¹é€²
1. **é¡å‹å®‰å…¨**: éœ€è¦å®Œæ•´çš„ type hints å’Œ mypy æª¢æŸ¥
2. **æ•ˆèƒ½å„ªåŒ–**: å¤§è¦æ¨¡æ•¸æ“šè™•ç†éœ€è¦å„ªåŒ–
3. **éŒ¯èª¤è™•ç†**: éœ€è¦æ›´åš´è¬¹çš„é‚Šç•Œæ¢ä»¶è™•ç†
4. **æ“´å±•æ€§**: éœ€è¦æŠ½è±¡å±¤æ”¯æ´å¤šæ˜Ÿç³»

### ğŸ¯ å»ºè­°é‡é»
1. **å…ˆä¿®å¾© P0 å•é¡Œ**: ç¢ºä¿åŸºç¤åŠŸèƒ½å¥å…¨
2. **éµå¾ª TDD**: æ–°åŠŸèƒ½å¿…é ˆå…ˆå¯«æ¸¬è©¦
3. **ä¿æŒè¦†è“‹ç‡**: æŒçºŒç¶­æŒ â‰¥ 90% æ¸¬è©¦è¦†è“‹
4. **æ¼¸é€²å¼é‡æ§‹**: é¿å…å¤§ç¯„åœé‡å¯«ï¼Œå°æ­¥è¿­ä»£

---

**å¯©æŸ¥å®Œæˆæ—¥æœŸ**: 2025-10-08
**ä¸‹æ¬¡å¯©æŸ¥å»ºè­°**: å¯¦ç¾æ–°åŠŸèƒ½å¾Œ (2 é€±å¾Œ)

**å¯©æŸ¥è€…ç°½å**: Claude Code (Senior Code Reviewer)
